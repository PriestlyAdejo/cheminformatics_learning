import random
import pandas as pd
from pathlib import Path
from utils import (
    LeadOptimizationPipeline, Specs, Protein, Ligand, 
    BindingSiteDetection, LigandSimilaritySearch, Docking, InteractionAnalysis, 
    OptimizedLigands, DataBaseIDMapper
)
from tqdm.auto import tqdm
from utils.helpers import (
    io
)
import shutil
from contextlib import contextmanager
import sys, os

"""
In the development of the docking process, we plan to integrate the predictions from the binding affinity prediction network. 
This integration is crucial because the docking step is the most computationally intensive part of the entire workflow. 
Utilizing the binding affinity network's predictions can significantly save computational resources compared to manually 
computing binding affinities through docking. 

The proposed approach involves adding an extra parameter in the docking function. This parameter will determine whether 
to obtain affinities and other relevant data directly from the Smina program or to use the predictive model for generating 
these results. Smina, our chosen tool, is utilized to dock ligands to the input protein and calculate affinities and other 
reaction-related data. 

The next step is to develop a model that can predict the outputs of Smina, assuming that the binding sites have already 
been identified in the previous step. The data generated by Smina during the docking results summary stage will be used 
to train this model to return more detailed information. In terms of implementation, this change would involve adding a 
new parameter to the Docking classâ€™s initialization method. This parameter would enable a choice between using the 
predictive model, which is less computationally intensive, or continuing with the original Smina docking program, which 
demands more computational power. This method ensures minimal changes to the existing codebase, reducing the amount of 
work needed for integration. 

Additionally, it's important to note that the subsequent stage of interaction analysis can independently make calculations 
based on the data derived from Smina or the predicted outcomes.
"""

            
# I think the visualisations (mostly from nglview) cannot be saved/rendered in basic python environments. I have to use an actual
# Jupyter notebook/jupyter lab to run the full script for everything so it accurately saves the data to correct viz directories

# TODO:
# 1. Create prints, displays and etc for each stage.
# 1.5 Also modify the printing of the molecules to include 3d representations of the ligands and proteins etc (from rdkit).
# 2. Convert prints, displays and etc into log files/outputs into the directory.
# 3. Change input folder structure so targets already seen are taken into consideration and information is not deleted.
# 4. Suppress relevant outputs (like fetching etc) and only display output if it is an error etc.
# 4. Apply tqdm package all over the project etc and instantiate a general logger for the entire teachopencadd pipeline.
# 5. Convert this entire thing into full run pipeline (with all above modifications made).
# 6. Run the full process from generation to finding analogs and check for errors.
# 7. Create relevant visualisations for the molecule generation stages etc and all its related parts etc.
# 7. Re organise the directory structure so its clean and ordered.

class AutomatedLeadOptimizationPipeline:
    def __init__(self, project_name, project_relpath, pipeline_input_data_df, data_folder="data"):
        with suppress_stdout():
            self.HERE = Path(__file__).parent
            self.DATA = self.HERE / data_folder 
            
            self.project_name = project_name
            self.project_relpath = project_relpath
            self.pipeline_input_data_df = pipeline_input_data_df

            # Need to instead input project path and then split into project name or supply both
            self.project = LeadOptimizationPipeline(project_name=self.project_name)
            
            self.min_affinity = -5
            self.min_total_num_interactions = 5
        
    def run(self):
        print(f"\nPIPELINE FOR PROJECT: {self.project_name}\n")
        self.process_input_data()
        self.process_protein_data()
        self.process_ligand_data()
        self.detect_binding_site()
        self.perform_ligand_similarity_search()
        self.execute_docking_calculations()
        self.analyze_interactions()
        self.select_best_analogs()
        self.save_best_analogs_to_file()
        
    def print_object_attributes(self, obj):
        for attr in dir(obj):
            # Filter out private and special methods
            if not attr.startswith("__"):
                try:
                    print(f"\n{attr}:\n{getattr(obj, attr)}")
                except Exception as e:
                    print(f"\n{attr}: Could not print value. Error: {e}")
    
    def display_input_data_stage(self):
        print("\n0. PRINTING SPECS STAGE AND PROCESSING INPUT INFORMATION:\n")
        print(f"\nINPUT DATA DF:\n{self.project.Specs.RawData.all_data}\n")
        
        # print(f"\nSPECS OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.Specs)
        print()
        
    def process_input_data(self):
        with suppress_stdout():
            # Create output folder for the project if it doesn't exist
            output_folder_path = self.DATA / "Outputs" / self.project_relpath
            output_df_path = output_folder_path / "0_INPUT_SPECS_DATA"
            io.create_folder(output_folder_path)
            io.create_folder(output_df_path)

            # Save the updated DataFrame in the output folder
            output_file_path = output_df_path / f"{self.project.name}_INPUT_SPECS_DATA.csv"
            self.pipeline_input_data_df.to_csv(output_file_path, index=False)

            # Update project specs with the new file path and output folder path
            self.project.Specs = Specs(output_file_path, output_folder_path)

        # Printing Information
        self.display_input_data_stage()

    def display_protein_data_stage(self):
        print("\n1. PROCESSING INPUT PROTEIN DATA:\n")
        print(f"\nSOME PROTEIN OBJECT ATTRIBUTES FROM CALL METHOD:\n{self.project.Protein()}\n")
        
        # print(f"\nALL PROTEIN OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.Protein)
        print()
        
    def process_protein_data(self):
        with suppress_stdout():
            self.project.Protein = Protein(
                identifier_type=self.project.Specs.Protein.input_type,
                identifier_value=self.project.Specs.Protein.input_value,
                protein_output_path=self.project.Specs.OutputPaths.protein,
            )
        
        # Printing Information
        self.display_protein_data_stage()
        
    def display_ligand_data_stage(self):
        print("\n2. PROCESSING INPUT LIGAND DATA:\n")
        print(f"\nLIGAND ATTRIBUTES DATAFRAME:\n{self.project.Ligand()}\n")
        
        # print(f"\nALL LIGAND OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.Ligand)
        print()
        
    def process_ligand_data(self):
        with suppress_stdout():
            self.project.Ligand = Ligand(
                identifier_type=self.project.Specs.Ligand.input_type,
                identifier_value=self.project.Specs.Ligand.input_value,
                ligand_output_path=self.project.Specs.OutputPaths.ligand,
            )
        
        # Printing Information
        self.display_ligand_data_stage()
    
    def display_binding_site_data_stage(self):
        print("\n3. DETECTING BINDING SITES:\n")
        print(f"\nBINDING SITES DATAFRAME:\n{self.project.BindingSiteDetection.dogsitescorer_binding_sites_df.head()}\n")
        print(f"\nBEST BINDING SITE NAME:\n{self.project.BindingSiteDetection.best_binding_site_name}\n")
        print(f"\nTHE BEST BINDING SITE COORDINATES:\n{self.project.Protein.binding_site_coordinates}\n")
        print(f"\nVISUALISATIONS OF ALL BINDING SITES:\n{self.project.BindingSiteDetection.visualize_all()}\n")
        print(f"\nVISUALISATION OF BEST BINDING SITE:\n{self.project.BindingSiteDetection.visualize_best()}\n")
        
        # print(f"\nALL BINDING SITES OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.BindingSiteDetection)
        print()
        
    def detect_binding_site(self):
        with suppress_stdout():
            self.project.BindingSiteDetection = BindingSiteDetection(
                protein_obj=self.project.Protein,
                binding_site_specs_obj=self.project.Specs.BindingSite,
                binding_site_output_path=self.project.Specs.OutputPaths.binding_site_detection,
            )

        # Printing Information
        self.display_binding_site_data_stage()
    
    def display_ligand_similarity_search_data_stage(self):
        print("\n4. PERFORMING LIGAND SIMILARITY SEARCH:\n")
        print(f"\nALL ANALOGS FOUND DATAFRAME:\n{self.project.LigandSimilaritySearch.all_analogs.head()}\n")
        print(f"\nALL ANALOGS FOUND DATAFRAME SHAPE:\n{self.project.LigandSimilaritySearch.all_analogs.shape}\n")
        print(f"\nMOST DRUGLIKE ANALOGS (ACCORDING TO INPUT SPECS):\n{self.project.Ligand.analogs}\n")
        
        # print(f"\nALL LIGAND SIMILARITY SEARCH OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.LigandSimilaritySearch)
        print()
        
    def perform_ligand_similarity_search(self):
        with suppress_stdout():
            self.project.LigandSimilaritySearch = LigandSimilaritySearch(
                ligand_obj=self.project.Ligand,
                similarity_search_specs_obj=self.project.Specs.LigandSimilaritySearch,
                similarity_search_output_path=self.project.Specs.OutputPaths.similarity_search,
            )
        
        # Printing Information
        self.display_ligand_similarity_search_data_stage()
    
    def display_docking_calculations_data_stage(self):
        best_affinity_pose = self.project.Docking.results_dataframe.sort_values(by="affinity[kcal/mol]").index[0]
        value_analog = self.project.Docking.results_dataframe.loc[best_affinity_pose]["affinity[kcal/mol]"]
        value_ligand = self.project.Ligand.dataframe_docking.loc[1]["affinity[kcal/mol]"]
        print("\n5. DOCKING ANALOGS AND INPUT LIGAND SEARCH:\n")
        print(f"\nDOCKING RESULTS FOR ALL ANALOGS DATAFRAME (SORTED BY AFFINITY TO INPUT PROTEIN):\n{self.project.Docking.results_dataframe.sort_values(by='affinity[kcal/mol]').head()}\n")
        print(f"\nDOCKING RESULTS FOR ALL ANALOGS DATAFRAME SHAPE:\n{self.project.Docking.results_dataframe.shape}\n")
        print(f"\nBEST RANKING ANALOG (CID): {best_affinity_pose[0]}, IS PREDICTED AFFINITY BELOW -10.0 [kcal/mol]?:{value_analog<-10.}.\n")
        print(f"\nBEST RANKING ANALOG DOCKING DATAFRAME:\n{self.project.Ligand.analogs[best_affinity_pose[0]].dataframe_docking}\n")
        print(f"\nBEST RANKING ANALOG SUMMARY OF RESULTS DATAFRAME:\n{self.project.Ligand.analogs[best_affinity_pose[0]]().tail(7)}\n")
        print(f"\nVISUALISING ALL ANALOG DOCKING POSES:\n{self.project.Docking.visualize_all_poses()}\n")
        print(f"\nINPUT LIGAND DOCKING DATAFRAME:\n{self.project.Ligand.dataframe_docking}\n")
        print(f"\nINPUT LIGAND SUMMARY OF RESULTS DATAFRAME:\n{self.project.Ligand.analogs[best_affinity_pose[0]]().tail(7)}\n")
        print(f"\nPREDICTED AFFINITY VALUE FOR POSE 1 OF INPUT LIGAND <= -8.5 [kcal/mol]: {value_ligand <= -8.5}.\n")
        
        # print(f"\nALL ANALOG DOCKING OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.Docking)
        # print(f"\nALL LIGAND DOCKING OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.Ligand.Docking)
        print()
        
    def execute_docking_calculations(self):
        # with suppress_stdout():
        self.project.Docking = Docking(
            protein_obj=self.project.Protein,
            list_ligand_obj=list(self.project.Ligand.analogs.values()),
            docking_specs_obj=self.project.Specs.Docking,
            docking_output_path=self.project.Specs.OutputPaths.docking,
            protein_pdb_code=self.project.Specs.Protein.pdb_code
        )
        
        self.project.Ligand.Docking = Docking(
            protein_obj=self.project.Protein,
            list_ligand_obj=[self.project.Ligand],
            docking_specs_obj=self.project.Specs.Docking,
            docking_output_path=self.project.Specs.OutputPaths.ligand,
            protein_pdb_code=self.project.Specs.Protein.pdb_code
        )
    
        # Printing Information
        self.display_docking_calculations_data_stage()
    
    def display_interactions_analyses_data_stage(self):
        # Calculations for all analog ligands
        best_interaction_and_score_df = self.project.InteractionAnalysis.results[
            self.project.InteractionAnalysis.results["affinity[kcal/mol]"] < self.min_affinity 
        ]
        best_interaction_and_score_df = best_interaction_and_score_df[
            best_interaction_and_score_df["total_num_interactions"] >= self.min_total_num_interactions
        ]
        best_pose_analogs = best_pose_analogs = best_interaction_and_score_df.index[0] if best_interaction_and_score_df.index.values.size > 0 else None
        value_analogs = best_interaction_and_score_df.loc[best_pose_analogs]["total_num_interactions"]

        # Printing and displaying values
        print("\n6. PERFORMING INTERACTION ANALYSES:\n")
        print(f"\nALL ANALOG-PROTEIN INTERACTIONS DATAFRAME:\n{self.project.InteractionAnalysis.results.sort_values(by='total_num_interactions', ascending=False).head()}\n")
        print(f"\nALL ANALOG-PROTEIN INTERACTIONS DATAFRAME SHAPE:\n{self.project.InteractionAnalysis.results.shape}\n")
        print(f"\nINPUT LIGAND-PROTEIN INTERACTIONS DATAFRAME:\n{self.project.Ligand.InteractionAnalysis.results.sort_values(by='total_num_interactions', ascending=False).head()}\n")
        print(f"\nALL INPUT LIGAND-PROTEIN INTERACTIONS DATAFRAME SHAPE:\n{self.project.Ligand.InteractionAnalysis.results.shape}\n")
        print(
            f"\nANALOG MOLECULE CID: {best_pose_analogs[0]} HAS "
            f"HIGHEST NUMBER OF INTERACTIONS >= {self.min_total_num_interactions}: "
            f"{value_analogs >= self.min_total_num_interactions}.\n"
        )
        print(f"\nANALOG WITH HIGHEST NUMBER OF INTERACTIONS:\n{self.project.Ligand.analogs[best_pose_analogs[0]]().tail(9)}\n")
        print(f"\nPLOTTING ALL INTERACTIONS:\n{self.project.InteractionAnalysis.plot_interaction_affinity_correlation()}\n")
        print(f"\nVISUALISING ALL INTERACTIONS:\n{self.project.InteractionAnalysis.visualize_all_interactions()}\n")
        
        # print(f"\nALL INTERACTION ANALYSIS OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.InteractionAnalysis)
        print()
        
    def analyze_interactions(self):
        with suppress_stdout():
            self.project.InteractionAnalysis = InteractionAnalysis(
                separated_protein_pdbqt_filepath=self.project.Docking.pdbqt_filepath_extracted_protein,
                separated_protein_pdb_filepath=self.project.Docking.pdb_filepath_extracted_protein,
                protein_first_residue_number=self.project.Protein.residue_number_first,
                list_ligand_obj=list(self.project.Ligand.analogs.values()),
                docking_master_df=self.project.Docking.master_df, # Docking master df is what would need to be recreated from predictions etc
                interaction_analysis_specs_obj=self.project.Specs.InteractionAnalysis,
                interaction_analysis_output_path=self.project.Specs.OutputPaths.interaction_analysis,
            )
            
            self.project.Ligand.InteractionAnalysis = InteractionAnalysis(
                separated_protein_pdbqt_filepath=self.project.Docking.pdbqt_filepath_extracted_protein,
                separated_protein_pdb_filepath=self.project.Docking.pdb_filepath_extracted_protein,
                protein_first_residue_number=self.project.Protein.residue_number_first,
                list_ligand_obj=[self.project.Ligand],
                docking_master_df=self.project.Docking.master_df,
                interaction_analysis_specs_obj=self.project.Specs.InteractionAnalysis,
                interaction_analysis_output_path=self.project.Specs.OutputPaths.ligand,
            )
        
        # Printing Information
        self.display_interactions_analyses_data_stage()
    
    def display_selecting_best_analogs_data_stage(self):
        # Printing and displaying values
        print("\n7. GETTING FINAL OPTIMISED LIGANDS:\n")
        print(f"\nOPTIMISED LIGANDS SUMMARY:\n{self.project.OptimizedLigands()}\n")
        print(f"\nDISPLAYING ANALOGS WITH HIGHER BINDING AFFINITY THAN INPUT LINGANDS:\n{self.project.OptimizedLigands.show_higher_affinity_analogs()}\n")
        print(f"\nDISPLAYING ANALOGS WITH HIGHER NUMBER OF PROTEIN-LIGAND INTERACTIONS THAN INPUT LINGANDS:\n{self.project.OptimizedLigands.show_higher_interacting_analogs()}\n")
        print(f"\nDISPLAYING ANALOGS WITH HIGHER BINDING AFFINITY AND PROTEIN-LIGAND INTERACTIONS THAN INPUT LINGANDS:\n{self.project.OptimizedLigands.show_higher_affinity_and_interacting_analogs()}\n") 
        print(f"\nDISPLAYING ANALOGS WITH HIGHER BINDING AFFINITY AND PROTEIN-LIGAND INTERACTIONS AND DRUG SCORES THAN INPUT LINGANDS:\n{self.project.OptimizedLigands.show_higher_affinity_and_interacting_and_druglike_analogs()}\n") 
        print(f"\nCIDS OF OPTIMISED LIGANDS:\n{self.project.OptimizedLigands.output}\n")
        
        # print(f"\nALL OPTIMISED LIGANDS OBJECT ATTRIBUTES:\n")
        # self.print_object_attributes(self.project.OptimizedLigands)
        print()

    def select_best_analogs(self):
        with suppress_stdout():
            self.project.OptimizedLigands = OptimizedLigands(self.project)
        
        # Printing Information
        self.display_selecting_best_analogs_data_stage()
        
        # Getting dataframes for all optimised analogs (as many as possible)
        self.optimised_analog_dfs = [ligand_obj() for ligand_obj in self.project.OptimizedLigands.output]
    
    def save_best_analogs_to_file(self):
        stage_name = "8_Optimized Ligands"
        for idx, df in enumerate(self.optimised_analog_dfs):
            name = df.loc['name', 'Value']
            cid = df.loc['cid', 'Value']
            project_path = self.DATA / "Outputs" / self.project_relpath
            
            # Define paths
            save_path = project_path / stage_name / f"{idx}_NAME_{name}_PDB_CID_{cid}"
            csv_filename = f"{idx}_NAME_{name}_PDB_CID_{cid}.csv"
            full_df_path = save_path / csv_filename

            # Create folder and save current DF
            io.create_folder(save_path)
            df.to_csv(full_df_path)

            # Traverse and copy relevant content
            for root, dirs, files in os.walk(project_path):
                for file in files:
                    if f"cid_{cid}" in file.lower() or f"cid_{cid}" in root.lower():
                        source_path = os.path.join(root, file)
                        relative_path = os.path.relpath(root, project_path)
                        if relative_path.startswith(stage_name):
                            continue # Skip copying from optimized ligands directory
                        if relative_path.split(os.sep)[0].isdigit():
                            # Remove any part of the path starting with numbers denoting stages of the process
                            relative_path_parts = [part for part in relative_path.split(os.sep)[1:]]
                            relative_path = os.path.join(*relative_path_parts)
                            
                        # Remove any part of the path containing 'cid_{cid}' and adjust for subdirectories
                        relative_path_parts = [part for part in relative_path.split(os.sep) if f"cid_{cid}" not in part.lower()]
                        relative_path = os.path.join(*relative_path_parts)
                    
                        destination_dir = save_path / relative_path
                        io.create_folder(destination_dir)
                        shutil.copy2(source_path, destination_dir)

@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:  
            yield
        finally:
            sys.stdout = old_stdout

def map_protein_id(protein_id):
    # Init Database ID Mapper
    mapper = DataBaseIDMapper()
    
    # PDB IDs are typically 4 characters long
    if len(protein_id) == 4:
        return protein_id  # It's likely a PDB ID, so use it as is

    # If the ID is longer, it's likely a UniProt ID and needs mapping
    else:
        conversion_dict = mapper.map_ids("UniProtKB_AC-ID", "PDB", [protein_id])
        mapped_ids = conversion_dict.get(protein_id)

        if not mapped_ids:
            raise ValueError(f"No PDB ID found for UniProt ID {protein_id}")

        # Choose a random PDB ID from the mapped IDs
        random_pdb_id = random.choice(mapped_ids)
        return random_pdb_id

def run_pipeline_for_each_smiles(protein_name, protein_id, protein_source, generated_smiles_data_path="generated_smiles_data.csv", input_data_template_filepath="InputData_Template.csv"):
    # Load SMILES data from file
    smiles_df = pd.read_csv(generated_smiles_data_path)
    druggable_smiles_df = smiles_df[smiles_df["is_druggable"]].reset_index(drop=True)
    
    if druggable_smiles_df.empty:
        raise ValueError("No druggable SMILES in input generated SMILES dataframe.")
    
    pipeline_input_data_df = pd.read_csv(input_data_template_filepath)
    mapped_protein_id = map_protein_id(protein_id)
    pipeline_input_data_df.loc[1, "Value"] = mapped_protein_id

    HERE = Path(__file__).parent
    output_root_path = HERE / "data" / "Outputs"
    project_base_path = output_root_path / f"Project_{protein_name}_{protein_source}_{protein_id}"
    io.create_folder(project_base_path)

    for _, row in druggable_smiles_df.iterrows():
        input_smiles_string = row['smiles']
        input_df_for_smiles_string = pipeline_input_data_df.copy()
        input_df_for_smiles_string.loc[3, "Value"] = input_smiles_string

        index = get_next_index_at(project_base_path)
        project_name = f"Project_{protein_name}_{protein_source}_{protein_id}_GENERATED_SMILES_{index}"
        project_relpath = Path(f"Project_{protein_name}_{protein_source}_{protein_id}") / f"GENERATED_SMILES_{index}"

        AutomatedLeadOptimizationPipeline(
            project_name=project_name,
            project_relpath=project_relpath,
            pipeline_input_data_df=input_df_for_smiles_string
        ).run() 
    
    # Need to do additional part here that orders all the ligands and all the smiles generated by their biding affinity 
    # into a new folder that ranks all the smiles ever generated and/or optimised for a particular protein and puts
    # them and their relevant information and properties into a dataframe etc.
    
    print("\n\nPipeline execution completed for all druggable SMILES.\n\n")

def get_next_index_at(project_path):
    max_index = -1
    for root, dirs, files in os.walk(project_path):
        for dir_name in dirs:
            if dir_name.startswith("GENERATED_SMILES_"):
                try:
                    index = int(dir_name.split('_')[-1])
                    max_index = max(max_index, index)
                except ValueError:
                    continue
    return max_index + 1
        

# Need to use parsers with this so I can just run on the command line and use previous stages of the pipeline to 
# do whats needed.
if __name__ == "__main__":
    protein_name = "FGFR_2"
    protein_id = "1GJO"
    protein_source = "PDB"
    HERE = Path(__file__).parent
    generated_smiles_data_path = HERE / "generated_smiles_data.csv"
    input_data_template_filepath = HERE / "InputData_Template.csv"

    run_pipeline_for_each_smiles(protein_name, protein_id, protein_source, generated_smiles_data_path=generated_smiles_data_path, input_data_template_filepath=input_data_template_filepath)
